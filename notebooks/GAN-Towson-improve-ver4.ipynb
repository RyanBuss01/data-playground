{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16bc02c3fb613f07",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Running Import Statements and ensuring GPU Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:59:42.674987Z",
     "start_time": "2024-04-19T07:59:39.282048Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory growth set\n",
      "GPU Device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \n",
      "\n",
      "Memory limit set to 8000MB on GPU /physical_device:GPU:0\n",
      "TensorFlow version: 2.10.0\n",
      "CUDA version: 64_112\n",
      "cuDNN version: 64_8\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] \n",
      " [LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:GPU:0', device_type='GPU')] \n",
      "\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "\n",
    "# List all physical devices and configure them before any other operations\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth on the GPU to true\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"Memory growth set\")\n",
    "            print(\"GPU Device:\", gpu, \"\\n\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before initializing the GPUs\n",
    "        print(\"RuntimeError in setting up GPU:\", e)\n",
    "        \n",
    "    try:\n",
    "        # Optional: Set a memory limit\n",
    "        memory_limit = 8000  # e.g., 4096 MB for 4GB\n",
    "        config = tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit)\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [config])\n",
    "        print(f\"Memory limit set to {memory_limit}MB on GPU {gpus[0].name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to set memory limit: {e}\")\n",
    "else:\n",
    "    print(\"No GPU devices found.\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import time\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Print versions and device configurations after ensuring GPU settings\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"CUDA version:\", tf.sysconfig.get_build_info()['cuda_version'])\n",
    "print(\"cuDNN version:\", tf.sysconfig.get_build_info()['cudnn_version'])\n",
    "print(tf.config.list_physical_devices(), \"\\n\", tf.config.list_logical_devices(), \"\\n\")\n",
    "print(tf.config.list_physical_devices('GPU'), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee08542e6ad9e0",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539315e1c513404b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:59:47.048426Z",
     "start_time": "2024-04-19T07:59:47.032022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part-00000-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00001-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00002-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00003-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00004-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00005-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00006-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00007-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00008-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00009-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00010-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00011-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00012-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00013-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00014-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00015-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00016-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00017-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00018-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00019-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00020-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00021-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00022-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00023-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00024-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00025-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00026-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00027-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00028-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00029-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00030-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00031-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00032-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00033-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00034-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00035-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00036-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00037-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00038-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00039-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00040-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00041-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00042-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00043-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00044-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00045-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00046-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00047-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00048-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00049-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00050-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00051-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00052-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00053-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00054-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00055-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00056-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00057-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00058-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00059-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00060-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00061-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00062-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00063-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00064-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00065-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00066-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00067-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00068-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00069-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00070-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00071-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00072-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00073-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00074-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00075-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00076-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00077-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00078-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00079-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00080-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00081-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00082-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00083-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00084-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00085-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00086-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00087-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00088-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00089-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00090-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00091-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00092-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00093-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00094-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00095-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00096-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00097-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00098-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00099-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00100-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00101-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00102-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00103-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00104-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00105-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00106-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00107-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00108-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00109-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00110-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00111-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00112-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00113-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00114-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00115-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00116-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00117-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00118-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00119-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00120-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00121-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00122-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00123-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00124-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00125-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00126-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00127-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00128-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00129-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00130-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00131-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00132-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00133-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00134-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00135-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00136-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00137-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00138-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00139-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00140-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00141-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00142-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00143-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00144-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00145-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00146-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00147-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00148-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00149-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00150-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00151-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00152-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00153-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00154-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00155-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00156-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00157-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00158-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00159-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00160-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00161-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00162-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00163-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00164-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00165-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00166-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00167-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00168-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv']\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIRECTORY = '../datasets/CICIoT2023/'          # If your dataset is within your python project directory, change this to the relative path to your dataset\n",
    "csv_filepaths = [filename for filename in os.listdir(DATASET_DIRECTORY) if filename.endswith('.csv')]\n",
    "\n",
    "print(csv_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbd3b980448fb21",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6107541190bd38e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:59:49.350961Z",
     "start_time": "2024-04-19T07:59:49.337717Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part-00042-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00137-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00118-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00167-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv']\n",
      "Training Sets:\n",
      " ['part-00042-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00118-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', 'part-00137-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv'] \n",
      "\n",
      "Test Sets:\n",
      " ['part-00167-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv']\n"
     ]
    }
   ],
   "source": [
    "csv_filepaths = [filename for filename in os.listdir(DATASET_DIRECTORY) if filename.endswith('.csv')]\n",
    "\n",
    "# If there are more than 40 CSV files, randomly select 40 files from the list\n",
    "sample_size = 4\n",
    "\n",
    "if len(csv_filepaths) > sample_size:\n",
    "    csv_filepaths = random.sample(csv_filepaths, sample_size)\n",
    "    print(csv_filepaths)\n",
    "\n",
    "csv_filepaths.sort()\n",
    "\n",
    "# Calculate the index for splitting the datasets into training (80%) and testing (20%)\n",
    "split_index = int(len(csv_filepaths) * 0.8)\n",
    "\n",
    "training_sets = csv_filepaths[:split_index]\n",
    "test_sets = csv_filepaths[split_index:]\n",
    "\n",
    "print(\"Training Sets:\\n\",training_sets, \"\\n\")\n",
    "print(\"Test Sets:\\n\",test_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d015a262bf3d40",
   "metadata": {},
   "source": [
    "### Labeling Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22923238d1d94197",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:59:52.270653Z",
     "start_time": "2024-04-19T07:59:52.252037Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_columns = [\n",
    "    'flow_duration', 'Header_Length', 'Protocol Type', 'Duration',\n",
    "       'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number',\n",
    "       'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
    "       'ece_flag_number', 'cwr_flag_number', 'ack_count',\n",
    "       'syn_count', 'fin_count', 'urg_count', 'rst_count', \n",
    "    'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
    "       'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min',\n",
    "       'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue',\n",
    "       'Radius', 'Covariance', 'Variance', 'Weight', \n",
    "]\n",
    "y_column = 'label'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9368c91949598c8",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4649c65a56bda03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:59:54.624153Z",
     "start_time": "2024-04-19T07:59:54.614504Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e610b644e9a1dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T07:59:59.137681Z",
     "start_time": "2024-04-19T07:59:56.979888Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "for train_set in tqdm(training_sets):\n",
    "    scaler.fit(pd.read_csv(DATASET_DIRECTORY + train_set)[X_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aaa3e725257871",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d06a4088dd1c50",
   "metadata": {},
   "source": [
    "### Classification: 8 (7+1) classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b693750ac071637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_7classes = {}\n",
    "dict_7classes['DDoS-RSTFINFlood'] = 'DDoS'\n",
    "dict_7classes['DDoS-PSHACK_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-SYN_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-UDP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-TCP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-ICMP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-SynonymousIP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-ACK_Fragmentation'] = 'DDoS'\n",
    "dict_7classes['DDoS-UDP_Fragmentation'] = 'DDoS'\n",
    "dict_7classes['DDoS-ICMP_Fragmentation'] = 'DDoS'\n",
    "dict_7classes['DDoS-SlowLoris'] = 'DDoS'\n",
    "dict_7classes['DDoS-HTTP_Flood'] = 'DDoS'\n",
    "\n",
    "dict_7classes['DoS-UDP_Flood'] = 'DoS'\n",
    "dict_7classes['DoS-SYN_Flood'] = 'DoS'\n",
    "dict_7classes['DoS-TCP_Flood'] = 'DoS'\n",
    "dict_7classes['DoS-HTTP_Flood'] = 'DoS'\n",
    "\n",
    "\n",
    "dict_7classes['Mirai-greeth_flood'] = 'Mirai'\n",
    "dict_7classes['Mirai-greip_flood'] = 'Mirai'\n",
    "dict_7classes['Mirai-udpplain'] = 'Mirai'\n",
    "\n",
    "dict_7classes['Recon-PingSweep'] = 'Recon'\n",
    "dict_7classes['Recon-OSScan'] = 'Recon'\n",
    "dict_7classes['Recon-PortScan'] = 'Recon'\n",
    "dict_7classes['VulnerabilityScan'] = 'Recon'\n",
    "dict_7classes['Recon-HostDiscovery'] = 'Recon'\n",
    "\n",
    "dict_7classes['DNS_Spoofing'] = 'Spoofing'\n",
    "dict_7classes['MITM-ArpSpoofing'] = 'Spoofing'\n",
    "\n",
    "dict_7classes['BenignTraffic'] = 'Benign'\n",
    "\n",
    "dict_7classes['BrowserHijacking'] = 'Web'\n",
    "dict_7classes['Backdoor_Malware'] = 'Web'\n",
    "dict_7classes['XSS'] = 'Web'\n",
    "dict_7classes['Uploading_Attack'] = 'Web'\n",
    "dict_7classes['SqlInjection'] = 'Web'\n",
    "dict_7classes['CommandInjection'] = 'Web'\n",
    "\n",
    "\n",
    "dict_7classes['DictionaryBruteForce'] = 'BruteForce'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d4e6a311cbe380",
   "metadata": {},
   "source": [
    "### Classification: 2 (1+1) Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7782f0784f9a666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_2classes = {}\n",
    "dict_2classes['DDoS-RSTFINFlood'] = 'Attack'\n",
    "dict_2classes['DDoS-PSHACK_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-SYN_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-UDP_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-TCP_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-ICMP_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-SynonymousIP_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-ACK_Fragmentation'] = 'Attack'\n",
    "dict_2classes['DDoS-UDP_Fragmentation'] = 'Attack'\n",
    "dict_2classes['DDoS-ICMP_Fragmentation'] = 'Attack'\n",
    "dict_2classes['DDoS-SlowLoris'] = 'Attack'\n",
    "dict_2classes['DDoS-HTTP_Flood'] = 'Attack'\n",
    "\n",
    "dict_2classes['DoS-UDP_Flood'] = 'Attack'\n",
    "dict_2classes['DoS-SYN_Flood'] = 'Attack'\n",
    "dict_2classes['DoS-TCP_Flood'] = 'Attack'\n",
    "dict_2classes['DoS-HTTP_Flood'] = 'Attack'\n",
    "\n",
    "\n",
    "dict_2classes['Mirai-greeth_flood'] = 'Attack'\n",
    "dict_2classes['Mirai-greip_flood'] = 'Attack'\n",
    "dict_2classes['Mirai-udpplain'] = 'Attack'\n",
    "\n",
    "dict_2classes['Recon-PingSweep'] = 'Attack'\n",
    "dict_2classes['Recon-OSScan'] = 'Attack'\n",
    "dict_2classes['Recon-PortScan'] = 'Attack'\n",
    "dict_2classes['VulnerabilityScan'] = 'Attack'\n",
    "dict_2classes['Recon-HostDiscovery'] = 'Attack'\n",
    "\n",
    "dict_2classes['DNS_Spoofing'] = 'Attack'\n",
    "dict_2classes['MITM-ArpSpoofing'] = 'Attack'\n",
    "\n",
    "dict_2classes['BenignTraffic'] = 'Benign'\n",
    "\n",
    "dict_2classes['BrowserHijacking'] = 'Attack'\n",
    "dict_2classes['Backdoor_Malware'] = 'Attack'\n",
    "dict_2classes['XSS'] = 'Attack'\n",
    "dict_2classes['Uploading_Attack'] = 'Attack'\n",
    "dict_2classes['SqlInjection'] = 'Attack'\n",
    "dict_2classes['CommandInjection'] = 'Attack'\n",
    "\n",
    "dict_2classes['DictionaryBruteForce'] = 'Attack'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63be9e76674fe225",
   "metadata": {},
   "source": [
    "### Custom Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "487488055587c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_categories = {\n",
    "#     'DDOS': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "#     'DOS': [18, 19, 20, 21],\n",
    "#     'Mirai':[23,24,25],\n",
    "#     'Spoofing':[16, 22],\n",
    "#     'Recon': [0,26, 27, 28, 29, 32],\n",
    "#     'Web':[0, 2, 30, 31, 33],\n",
    "#     'BruteForce':[17],\n",
    "#     'Benign':[1]\n",
    "# }\n",
    "# \n",
    "# include_label_categories = []\n",
    "# include_labels = []\n",
    "# \n",
    "# for category in include_label_categories:\n",
    "#     for label in label_categories[category]:\n",
    "#         include_labels.append(label)\n",
    "# \n",
    "# excluded_records = df[~df['labels'].isin(include_labels)]\n",
    "# df = df[df['labels'].isin(include_labels)]\n",
    "# \n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a63563bc6cf116",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56d467f7b9d11e",
   "metadata": {},
   "source": [
    "### Preloading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daf799f758f9a633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:01:01.639483Z",
     "start_time": "2024-04-19T08:00:59.154929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set part-00042-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv out of 3 \n",
      "\n",
      "Training set part-00118-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv out of 3 \n",
      "\n",
      "Training set part-00137-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv out of 3 \n",
      "\n",
      "Real data:\n",
      "        flow_duration  Header_Length  Protocol Type  Duration      Rate  \\\n",
      "53700       -0.020617      -0.165078      -0.329392  -0.16746 -0.090465   \n",
      "101916      -0.020877      -0.165208      -0.903509  -0.16746 -0.090473   \n",
      "\n",
      "           Srate    Drate  fin_flag_number  syn_flag_number  rst_flag_number  \\\n",
      "53700  -0.090465 -0.00517         3.250179        -0.509118         3.170321   \n",
      "101916 -0.090473 -0.00517        -0.307675        -0.509118        -0.315426   \n",
      "\n",
      "        ...       Std  Tot size       IAT    Number  Magnitue    Radius  \\\n",
      "53700   ... -0.206841 -0.290654  0.010439  0.002633 -0.314537 -0.206668   \n",
      "101916  ... -0.208109 -0.340652  0.018818  0.002633 -0.457159 -0.207937   \n",
      "\n",
      "        Covariance  Variance   Weight             label  \n",
      "53700    -0.100493  0.319799  0.00264  DDoS-RSTFINFlood  \n",
      "101916   -0.100494 -0.412440  0.00264   DDoS-ICMP_Flood  \n",
      "\n",
      "[2 rows x 47 columns]\n",
      "length of features 46\n",
      "[ 9  6 13 ... 23 20 14]\n",
      "Data fully processed, shapes: (698733, 46) (698733,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "preload the data\n",
    "\"\"\"\n",
    "full_data = pd.DataFrame()\n",
    "for train_set in training_sets:\n",
    "    print(f\"Training set {train_set} out of {len(training_sets)} \\n\")\n",
    "    data_path = os.path.join(DATASET_DIRECTORY, train_set)\n",
    "    df = pd.read_csv(data_path)\n",
    "    full_data = pd.concat([full_data, df])\n",
    "\n",
    "# Shuffle data\n",
    "full_data = shuffle(full_data, random_state=1)\n",
    "\n",
    "# Scale the features\n",
    "full_data[X_columns] = scaler.transform(full_data[X_columns])\n",
    "\n",
    "\n",
    "#to visually judge results\n",
    "print(\"Real data:\")\n",
    "print(full_data[:2])\n",
    "\n",
    "# Convert DataFrame to NumPy array to facilitate batch operations\n",
    "X_train = full_data[X_columns].values\n",
    "y_train = full_data[y_column].values\n",
    "\n",
    "print(\"length of features\",len(X_train[0]))\n",
    "\"\"\"\n",
    "Redundant Label Encoding\n",
    "\"\"\"\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "print(y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Data fully processed, shapes:\", X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b3b7de14e652cc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Updated GAN Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e440bc51ca32170f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_generator(hidden1, hidden2, hidden3):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden1, input_dim=46))  # arbitrarily selected 100 for our input noise vector?\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(hidden2))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(hidden3))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(41, activation='relu'))  # outputs a generated vector of the same size as our data (41)\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    # input output defs\n",
    "    noise = Input(shape=(46,))\n",
    "    attack = model(noise)\n",
    "    # noise is input, attack is the output from the model object\n",
    "    return Model(noise, attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f3eb8282be85bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(46, input_dim=46, activation='relu'))  # discriminator takes 41 values from our dataset\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # outputs 0 to 1, 1 being read and 0 being fake\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    attack = Input(shape=(46,))\n",
    "    validity = model(attack)\n",
    "\n",
    "    return Model(attack, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32920f00a3cf4e97",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfa3d04b335184e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGAN(gen_hidden1, gen_hidden2, gen_hidden3, X_train):\n",
    "    batch_size = 1200\n",
    "    epochs = 7000\n",
    "    optimizer = Adam(0.0002, 0.5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # labels for data. 1 for valid attacks, 0 for fake (generated) attacks\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    # build the discriminator portion\n",
    "    discriminator = build_discriminator()\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    # build the generator portion\n",
    "    generator = build_generator(gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "    \n",
    "    #input and output of our combined model\n",
    "    z = Input(shape=(len(X_train[0]),))\n",
    "    attack = generator(z)\n",
    "    validity = discriminator(attack)\n",
    "    \n",
    "    # build combined model from generator and discriminator\n",
    "    combined = Model(z, validity)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    #break condition for training (when diverging)\n",
    "    loss_increase_count = 0\n",
    "    prev_g_loss = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        # selecting batch_size random attacks from our training data\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        attacks = X_train[idx]\n",
    "        \n",
    "        # generate a matrix of noise vectors\n",
    "        noise = np.random.normal(0, 1, (batch_size, 41))\n",
    "        \n",
    "        # create an array of generated attacks\n",
    "        gen_attacks = generator.predict(noise)\n",
    "        \n",
    "        # loss functions, based on what metrics we specify at model compile time\n",
    "        d_loss_real = discriminator.train_on_batch(attacks, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_attacks, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        # generator loss function\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f] [Loss change: %.3f, Loss increases: %.0f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss, g_loss - prev_g_loss, loss_increase_count))\n",
    "        \n",
    "        # if our generator loss icreased this iteration, increment the counter by 1\n",
    "        if (g_loss - prev_g_loss) > 0:\n",
    "            loss_increase_count = loss_increase_count + 1\n",
    "        else: \n",
    "            loss_increase_count = 0  # otherwise, reset it to 0, we are still training effectively\n",
    "            \n",
    "        prev_g_loss = g_loss\n",
    "            \n",
    "        if loss_increase_count > 5:\n",
    "            print('Stoping on iteration: ', epoch)\n",
    "            break\n",
    "            \n",
    "        if epoch % 20 == 0:\n",
    "            f = open(\"GANresultsNeptune.txt\", \"a\")\n",
    "            np.savetxt(\"GANresultsNeptune.txt\", gen_attacks, fmt=\"%.0f\")\n",
    "            f.close()\n",
    "            \n",
    "    # peek at our results\n",
    "    results = np.loadtxt(\"GANresultsNeptune.txt\")\n",
    "    print(\"Generated Neptune attacks: \")\n",
    "    print(results[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26106e7d66f68152",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82b6150bc0837309",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set part-00167-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv out of 1 \n",
      "\n",
      "Real data:\n",
      "        flow_duration  Header_Length  Protocol Type  Duration     Rate  \\\n",
      "203154      -0.020945      -0.165208      -1.054257 -4.753343 -0.09048   \n",
      "90847       -0.020229      -0.165208      -1.054257 -4.753343 -0.09048   \n",
      "\n",
      "          Srate      Drate  fin_flag_number  syn_flag_number  rst_flag_number  \\\n",
      "203154 -0.09048 -19.862585        -1.402339        -1.768319        -1.414919   \n",
      "90847  -0.09048 -19.862585        -1.402339         4.348888        -1.414919   \n",
      "\n",
      "        ...       Std  Tot size       IAT     Number  Magnitue    Radius  \\\n",
      "203154  ... -0.209433 -0.517442 -4.895001 -11.636403 -1.564974 -0.208872   \n",
      "90847   ... -0.209418 -0.517428 -4.895001 -11.636403 -1.564862 -0.208862   \n",
      "\n",
      "        Covariance  Variance    Weight                    label  \n",
      "203154   -0.100494 -2.188939 -6.740261        DDoS-PSHACK_Flood  \n",
      "90847    -0.100494 -1.817884 -6.740261  DDoS-SynonymousIP_Flood  \n",
      "\n",
      "[2 rows x 47 columns]\n",
      "length of features 46\n",
      "[ 8 12 21 ... 10  9  6]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_test)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# apply \"le.fit_transform\" to every column (usually only works on 1 column)\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m dataframe_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mdataframe\u001b[49m\u001b[38;5;241m.\u001b[39mapply(le\u001b[38;5;241m.\u001b[39mfit_transform)\n\u001b[0;32m     36\u001b[0m attack_labels \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m     37\u001b[0m indices_of_neptune \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(attack_labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneptune.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "preload the data\n",
    "\"\"\"\n",
    "full_test_data = pd.DataFrame()\n",
    "for test_set in test_sets:\n",
    "    print(f\"Training set {test_set} out of {len(test_sets)} \\n\")\n",
    "    data_path = os.path.join(DATASET_DIRECTORY, test_set)\n",
    "    df = pd.read_csv(data_path)\n",
    "    full_test_data = pd.concat([full_data, df])\n",
    "\n",
    "# Shuffle data\n",
    "full_test_data = shuffle(full_data, random_state=1)\n",
    "\n",
    "# Scale the features\n",
    "full_test_data[X_columns] = scaler.transform(full_test_data[X_columns])\n",
    "\n",
    "\n",
    "#to visually judge results\n",
    "print(\"Real data:\")\n",
    "print(full_test_data[:2])\n",
    "\n",
    "# Convert DataFrame to NumPy array to facilitate batch operations\n",
    "X_test = full_test_data[X_columns].values\n",
    "y_test = full_test_data[y_column].values\n",
    "\n",
    "print(\"length of features\",len(X_train[0]))\n",
    "\"\"\"\n",
    "Redundant Label Encoding\n",
    "\"\"\"\n",
    "le = LabelEncoder()\n",
    "y_test = le.fit_transform(y_test)\n",
    "print(y_test)\n",
    "\n",
    "# apply \"le.fit_transform\" to every column (usually only works on 1 column)\n",
    "dataframe_encoded = dataframe.apply(le.fit_transform)\n",
    "attack_labels = le.classes_\n",
    "indices_of_neptune = np.where(attack_labels == 'neptune.')\n",
    "neptune_index = indices_of_neptune[0]\n",
    "dataset = dataframe_encoded.values\n",
    "\n",
    "print(\"Data fully processed, shapes:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6995f4eee9f52c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# RUN GAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b38ec80b1a28cac2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_test)\n",
    "encoded_Y = encoder.transform(y_test)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "# print(dummy_y)\n",
    "#print(len(dummy_y[0]))\n",
    "num_of_classes = len(dummy_y[0])  # the length of dummy y is the number of classes we have in our small sample\n",
    "# since we are randomly sampling from a large dataset, we might not get 1 of every class in our sample\n",
    "# we need to set output layer to be equal to the length of our dummy_y vectors\n",
    "print(num_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f19b5928dd91b7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6995ec92cf76edf9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    inputs = 41\n",
    "    hidden_layer1 = 10\n",
    "    hidden_layer2 = 5\n",
    "    hidden_layer3 = 0\n",
    "    outputs = num_of_classes  #needs to be this variable in case we forget to sample. Could end up having 10 classes or 12, etc\n",
    "    \n",
    "    model.add(Dense(hidden_layer1, input_dim=inputs, activation='relu'))\n",
    "    if hidden_layer2 != 0:\n",
    "        model.add(Dense(hidden_layer2, activation='relu'))\n",
    "    if hidden_layer3 != 0:\n",
    "        model.add(Dense(hidden_layer3, activation='relu'))\n",
    "    model.add(Dense(outputs, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #optimizer=adam\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42fa4db54138ec5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "trainGAN() missing 1 required positional argument: 'X_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# create a unique filename in case we want to store the results (good accuracy)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     result_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../Results/GANresultsNeptune\u001b[39m\u001b[38;5;132;01m%.0f\u001b[39;00m\u001b[38;5;132;01m%.0f\u001b[39;00m\u001b[38;5;132;01m%.0f\u001b[39;00m\u001b[38;5;124miter\u001b[39m\u001b[38;5;132;01m%.0f\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (gen_hidden1, gen_hidden2, gen_hidden3, i)\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mtrainGAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_hidden1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_hidden2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_hidden3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# load generate attacks from file\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     results \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../Results/GANresultsNeptune.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: trainGAN() missing 1 required positional argument: 'X_train'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "f = open(\"GeneratorHypersAbove50percentAccuracy.txt\", \"w\")\n",
    "f.write(\"\"\"\"\"\" Hidden layer counts for Generator model that resulted in over 50% generated attacks labeled correctly:\n",
    "    ------------------------------------------------------------------------------------------------\n",
    "    \"\"\"\"\"\")\n",
    "f.close()\n",
    "\"\"\"\n",
    "\n",
    "while(1):\n",
    "    # generate random numbers for the hidden layer sizes of our generator\n",
    "    gen_hidden1 = 32\n",
    "    gen_hidden2 = 16\n",
    "    gen_hidden3 = 8\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    \n",
    "    # train 5 times on each setup, in case we get unlucky initalization on an otherwise good setup\n",
    "    while i < 5:\n",
    "        # create a unique filename in case we want to store the results (good accuracy)\n",
    "        result_filename = \"../../Results/GANresultsNeptune%.0f%.0f%.0fiter%.0f.txt\" % (gen_hidden1, gen_hidden2, gen_hidden3, i)\n",
    "\n",
    "        trainGAN(gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "        \n",
    "        # load generate attacks from file\n",
    "        results = np.loadtxt(\"../../Results/GANresultsNeptune.txt\")\n",
    "\n",
    "        # predict attack lables (as encoded integers)\n",
    "        y_pred = estimator.predict(results)\n",
    "        print(y_pred)\n",
    "\n",
    "        # create appropriate labels for our generated neptune attacks\n",
    "        neptune_labels = np.full((len(results),), neptune_index[0])\n",
    "\n",
    "        # convert integer labels back to string, get all unique strings and their count\n",
    "        predicted_as_label = attack_labels[y_pred]\n",
    "        unique_labels = np.unique(predicted_as_label)\n",
    "\n",
    "        for label in unique_labels:\n",
    "            print(\"Attack type: %s     number predicted:  %.0f\" % (label, len(np.where(predicted_as_label == label)[0])))\n",
    "    \n",
    "        print()\n",
    "        # create a confusion matrix of the results\n",
    "        cm = confusion_matrix(neptune_labels, y_pred)\n",
    "        \n",
    "        accuracy = np.trace(cm) / cm.sum()\n",
    "        print(cm)\n",
    "        print(\"total: \" + str(cm.sum()))\n",
    "        print(\"accuracy: \" + str(accuracy))\n",
    "        \n",
    "        if accuracy > .50:\n",
    "            f = open(\"../../Results/GeneratorHypersAbove50percentAccuracyNeptune.txt\", \"a\")\n",
    "            f.write(\"\"\"\n",
    "            \n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "Accuracy: %.3f\n",
    "Generator hidden layer 1 size: %.0f\n",
    "Generator hidden layer 2 size: %.0f\n",
    "Generator hidden layer 3 size: %.0f\n",
    "Iteration %.0f\n",
    "Result file name: %s\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\" % (accuracy, gen_hidden1, gen_hidden2, gen_hidden3, i, result_filename))\n",
    "            f.close()\n",
    "            result_filename = \"../../Results/\" + result_filename\n",
    "            \n",
    "            f = open(result_filename, \"w\")\n",
    "            f.close()\n",
    "            np.savetxt(result_filename, results, fmt=\"%.0f\")\n",
    "        \n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e6d9c140d7dff",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe8ec4c379f2369",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator_save_path = \"C:\\\\Users\\\\kskos\\\\PycharmProjects\\\\CEN-3078-Class-Project-Balancing-Gan-Algorithm-for-cyber-attack-datasets\\\\model\\\\generator\"\n",
    "discriminator_save_path = \"C:\\\\Users\\\\kskos\\\\PycharmProjects\\\\CEN-3078-Class-Project-Balancing-Gan-Algorithm-for-cyber-attack-datasets\\\\model\\\\discriminator\"\n",
    "\n",
    "# Save the generator\n",
    "gan.generator.save(generator_save_path)\n",
    "# Save the discriminator\n",
    "gan.discriminator.save(discriminator_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9e36b9e08ee5f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d47a79ec79641d9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator_load_path = \"/model/generator\"\n",
    "discriminator_load_path = \"/model/discriminator\"\n",
    "\n",
    "gan.generator = load_model(generator_load_path)\n",
    "gan.discriminator = load_model(discriminator_load_path)\n",
    "\n",
    "gan.generator.summary()\n",
    "gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994216370e2cbc3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Test Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c8dc981fd83dae",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Will continue to run until a better model is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054a1c9e3dbf41a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Looper:\n",
    "    def random_numbers(self):\n",
    "        gen_hidden1 = np.random.randint(1, 101)\n",
    "        gen_hidden2 = np.random.randint(1, 101)\n",
    "        gen_hidden3 = np.random.randint(1, 101)\n",
    "        return [gen_hidden1, gen_hidden2, gen_hidden3]\n",
    "    \n",
    "    def evaluate(gan):\n",
    "        noise = tf.random.normal((num_samples, input_shape))\n",
    "        generated_samples = gan.generator(noise)\n",
    "        discriminator_predictions = gan.discriminator.predict(generated_samples)\n",
    "        ideal_output = np.ones((num_samples,))\n",
    "        discriminator_predictions_rounded = np.round(discriminator_predictions).flatten()\n",
    "        ideal_output = np.ones((num_samples,))\n",
    "        accuracy = accuracy_score(ideal_output, discriminator_predictions_rounded)\n",
    "        f1 = f1_score(ideal_output, discriminator_predictions_rounded)\n",
    "        return accuracy, f1\n",
    "    \n",
    "    def save(gan):\n",
    "        generator_save_path = \"model/best_generator\"\n",
    "        discriminator_save_path = \"model/best_discriminator\"\n",
    "        gan.generator.save(generator_save_path)\n",
    "        gan.discriminator.save(discriminator_save_path)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fedb5448092be35",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Final Reuslt From Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adfbebb93ec143",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "f = open(\"GeneratorHypersAbove50percentAccuracy.txt\", \"w\")\n",
    "f.write(\"\"\"\"\"\" Hidden layer counts for Generator model that resulted in over 50% generated attacks labeled correctly:\n",
    "    ------------------------------------------------------------------------------------------------\n",
    "    \"\"\"\"\"\")\n",
    "f.close()\n",
    "\"\"\"\n",
    "\n",
    "while(1):\n",
    "    # generate random numbers for the hidden layer sizes of our generator\n",
    "    gen_hidden1 =  np.random.randint(1, 101)\n",
    "    gen_hidden2 =  np.random.randint(1, 101)\n",
    "    gen_hidden3 =  np.random.randint(1, 101)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    \n",
    "    # train 5 times on each setup, in case we get unlucky initalization on an otherwise good setup\n",
    "    while i < 100:\n",
    "        # create a unique filename in case we want to store the results (good accuracy)\n",
    "        result_filename = \"../../Results2/GANresultsportsweep%.0f%.0f%.0fiter%.0ftry2.txt\" % (gen_hidden1, gen_hidden2, gen_hidden3, i)\n",
    "\n",
    "        trainGAN(gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "        \n",
    "        # load generate attacks from file\n",
    "        results = np.loadtxt(\"../../Results2/GANresultsportsweep.txt\")\n",
    "\n",
    "        # predict attack lables (as encoded integers)\n",
    "        y_pred = estimator.predict(results)\n",
    "        print(y_pred)\n",
    "\n",
    "        # create appropriate labels for our generated portsweep attacks\n",
    "        portsweep_labels = np.full((len(results),), portsweep_index[0])\n",
    "\n",
    "        # convert integer labels back to string, get all unique strings and their count\n",
    "        predicted_as_label = attack_labels[y_pred]\n",
    "        unique_labels = np.unique(predicted_as_label)\n",
    "\n",
    "        for label in unique_labels:\n",
    "            print(\"Attack type: %s     number predicted:  %.0f\" % (label, len(np.where(predicted_as_label == label)[0])))\n",
    "    \n",
    "        print()\n",
    "        # create a confusion matrix of the results\n",
    "        cm = confusion_matrix(portsweep_labels, y_pred)\n",
    "        \n",
    "        accuracy = np.trace(cm) / cm.sum()\n",
    "        print(cm)\n",
    "        print(\"total: \" + str(cm.sum()))\n",
    "        print(\"accuracy: \" + str(accuracy))\n",
    "        \n",
    "        if accuracy > .50:\n",
    "            f = open(\"../../Results2/GeneratorHypersAbove50percentAccuracyportsweep.txt\", \"a\")\n",
    "            f.write(\"\"\"\n",
    "            \n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "Accuracy: %.3f\n",
    "Generator hidden layer 1 size: %.0f\n",
    "Generator hidden layer 2 size: %.0f\n",
    "Generator hidden layer 3 size: %.0f\n",
    "Iteration %.0f\n",
    "Result file name: %s\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\" % (accuracy, gen_hidden1, gen_hidden2, gen_hidden3, i, result_filename))\n",
    "            f.close()\n",
    "            result_filename = result_filename\n",
    "            \n",
    "            f = open(result_filename, \"w\")\n",
    "            f.close()\n",
    "            np.savetxt(result_filename, results, fmt=\"%.0f\")\n",
    "        \n",
    "        i = i + 1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c6f2478ba0be7",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
