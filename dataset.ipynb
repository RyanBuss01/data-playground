{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Psuedo Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function build_discriminator:\n",
    "    Initialize a sequential model\n",
    "    Add a dense layer with 41 units, input dimension 41, and 'relu' activation to the model\n",
    "    Add a dense layer with 30 units and 'relu' activation to the model\n",
    "    Add a dense layer with 15 units and 'relu' activation to the model\n",
    "    Add a dense layer with 1 unit and 'sigmoid' activation to the model\n",
    "    Define an input 'attack' with shape 41\n",
    "    Pass 'attack' through the model to get 'validity'\n",
    "    Return a model with input 'attack' and output 'validity'\n",
    "End Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(41, input_dim=41, activation='relu'))  # discriminator takes 41 values from our dataset\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # outputs 0 to 1, 1 being read and 0 being fake\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    attack = Input(shape=(41,))\n",
    "    validity = model(attack)\n",
    "\n",
    "    return Model(attack, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Psuedo Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function build_generator with parameters hidden1, hidden2, hidden3:\n",
    "    Initialize a sequential model\n",
    "    Add a dense layer with 'hidden1' units, input dimension 41 to the model\n",
    "    Add a LeakyReLU layer with alpha 0.2 to the model\n",
    "    Add a BatchNormalization layer with momentum 0.8 to the model\n",
    "    Add a dense layer with 'hidden2' units to the model\n",
    "    Add a LeakyReLU layer with alpha 0.2 to the model\n",
    "    Add a BatchNormalization layer with momentum 0.8 to the model\n",
    "    Add a dense layer with 'hidden3' units to the model\n",
    "    Add a LeakyReLU layer with alpha 0.2 to the model\n",
    "    Add a BatchNormalization layer with momentum 0.8 to the model\n",
    "    Add a dense layer with 41 units and 'relu' activation to the model\n",
    "    Define an input 'noise' with shape 41\n",
    "    Pass 'noise' through the model to get 'attack'\n",
    "    Return a model with input 'noise' and output 'attack'\n",
    "End Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_generator(hidden1, hidden2, hidden3):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden1, input_dim=41))  # arbitrarily selected 100 for our input noise vector?\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(hidden2))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(hidden3))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(41, activation='relu'))  # outputs a generated vector of the same size as our data (41)\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    noise = Input(shape=(41,))\n",
    "    attack = model(noise)\n",
    "    return Model(noise, attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudocode for trainGAN function\n",
    "\n",
    "1. Define function trainGAN with parameters gen_hidden1, gen_hidden2, gen_hidden3\n",
    "2. Set batch_size, epochs, and optimizer\n",
    "3. Load and sample data from the CSV file\n",
    "4. Encode the categorical data in the dataframe\n",
    "5. Print some real attacks for visual inspection\n",
    "6. Split the dataset into features (X_train) and labels (Y_train)\n",
    "7. Define labels for valid and fake attacks\n",
    "8. Build and compile the discriminator\n",
    "9. Build the generator\n",
    "10. Define the input and output for the combined model\n",
    "11. Build and compile the combined model\n",
    "12. Initialize variables for tracking the generator's loss\n",
    "13. For each epoch in the range of epochs:\n",
    "    1. Select a batch of real attacks from the training data\n",
    "    2. Generate a batch of fake attacks\n",
    "    3. Train the discriminator on both real and fake attacks\n",
    "    4. Train the generator to try to fool the discriminator\n",
    "    5. Print the losses of the discriminator and generator every 100 epochs\n",
    "    6. If the generator's loss has increased for more than 5 consecutive epochs, stop the training\n",
    "    7. Every 20 epochs, save the generated attacks to a text file\n",
    "14. Load the generated attacks from the text file and print some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGAN(gen_hidden1, gen_hidden2, gen_hidden3):\n",
    "    batch_size = 256\n",
    "    epochs = 7000\n",
    "    optimizer = Adam(0.0002, 0.5)\n",
    "    \n",
    "    dataframe = pd.read_csv('../../CSV/portsweep.csv').sample(500) # sample 100 data points randomly from the csv\n",
    "    \n",
    "    # apply \"le.fit_transform\" to every column (usually only works on 1 column)\n",
    "    le = LabelEncoder()\n",
    "    dataframe_encoded = dataframe.apply(le.fit_transform)\n",
    "    dataset = dataframe_encoded.values\n",
    "    \n",
    "    #to visually judge results\n",
    "    print(\"Real portsweep attacks:\")\n",
    "    print(dataset[:2])\n",
    "    \n",
    "    # Set X as our input data and Y as our label\n",
    "    X_train = dataset[:, 0:41].astype(float)\n",
    "    Y_train = dataset[:, 41]\n",
    "    \n",
    "    # labels for data. 1 for valid attacks, 0 for fake (generated) attacks\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    # build the discriminator portion\n",
    "    discriminator = build_discriminator();\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    # build the generator portion\n",
    "    generator = build_generator(gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "    \n",
    "    #input and output of our combined model\n",
    "    z = Input(shape=(41,))\n",
    "    attack = generator(z)\n",
    "    validity = discriminator(attack)\n",
    "    \n",
    "    # build combined model from generator and discriminator\n",
    "    combined = Model(z, validity)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    #break condition for training (when diverging)\n",
    "    loss_increase_count = 0;\n",
    "    prev_g_loss = 0;\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        # selecting batch_size random attacks from our training data\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        attacks = X_train[idx]\n",
    "        \n",
    "        # generate a matrix of noise vectors\n",
    "        noise = np.random.normal(0, 1, (batch_size, 41))\n",
    "        \n",
    "        # create an array of generated attacks\n",
    "        gen_attacks = generator.predict(noise)\n",
    "        \n",
    "        # loss functions, based on what metrics we specify at model compile time\n",
    "        d_loss_real = discriminator.train_on_batch(attacks, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_attacks, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        # generator loss function\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f] [Loss change: %.3f, Loss increases: %.0f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss, g_loss - prev_g_loss, loss_increase_count))\n",
    "        \n",
    "        # if our generator loss icreased this iteration, increment the counter by 1\n",
    "        if (g_loss - prev_g_loss) > 0:\n",
    "            loss_increase_count = loss_increase_count + 1\n",
    "        else: \n",
    "            loss_increase_count = 0  # otherwise, reset it to 0, we are still training effectively\n",
    "            \n",
    "        prev_g_loss = g_loss\n",
    "            \n",
    "        if loss_increase_count > 5:\n",
    "            print('Stoping on iteration: ', epoch)\n",
    "            break\n",
    "            \n",
    "        if epoch % 20 == 0:\n",
    "            f = open(\"../../Results/GANresultsportsweep.txt\", \"a\")\n",
    "            np.savetxt(\"../../Results/GANresultsportsweep.txt\", gen_attacks, fmt=\"%.0f\")\n",
    "            f.close()\n",
    "            \n",
    "    # peek at our results\n",
    "    results = np.loadtxt(\"../../Results/GANresultsportsweep.txt\")\n",
    "    print(\"Generated portsweep attacks: \")\n",
    "    print(results[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *** Pre Proccess ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function baseline_model:\n",
    "    Initialize a sequential model\n",
    "    Define 'inputs' as 41\n",
    "    Define 'hidden_layer1' as 10\n",
    "    Define 'hidden_layer2' as 5\n",
    "    Define 'hidden_layer3' as 0\n",
    "    Define 'outputs' as the number of classes\n",
    "    Add a dense layer with 'hidden_layer1' units, input dimension 'inputs', and 'relu' activation to the model\n",
    "    If 'hidden_layer2' is not 0:\n",
    "        Add a dense layer with 'hidden_layer2' units and 'relu' activation to the model\n",
    "    If 'hidden_layer3' is not 0:\n",
    "        Add a dense layer with 'hidden_layer3' units and 'relu' activation to the model\n",
    "    Add a dense layer with 'outputs' units and 'softmax' activation to the model\n",
    "    Compile the model with 'categorical_crossentropy' loss, 'adam' optimizer, and 'accuracy' as a metric\n",
    "    Return the model\n",
    "End Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    inputs = 41\n",
    "    hidden_layer1 = 10\n",
    "    hidden_layer2 = 5\n",
    "    hidden_layer3 = 0\n",
    "    outputs = num_of_classes  #needs to be this variable in case we forget to sample. Could end up having 10 classes or 12, etc\n",
    "    \n",
    "    model.add(Dense(hidden_layer1, input_dim=inputs, activation='relu'))\n",
    "    if hidden_layer2 != 0:\n",
    "        model.add(Dense(hidden_layer2, activation='relu'))\n",
    "    if hidden_layer3 != 0:\n",
    "        model.add(Dense(hidden_layer3, activation='relu'))\n",
    "    model.add(Dense(outputs, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #optimizer=adam\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a KerasClassifier 'estimator' with the function 'baseline_model', 32 epochs, batch size 200, and verbosity level 2\n",
    "Initialize a KFold 'kfold' with 10 splits, shuffle enabled, and a random state 'seed'\n",
    "Predict the cross-validated outputs 'y_pred' for the estimator on inputs 'X' and labels 'dummy_y' with cross-validation 'kfold'\n",
    "Calculate the cross-validation score 'results' for the estimator on inputs 'X' and labels 'dummy_y' with cross-validation 'kfold'\n",
    "Fit the estimator on inputs 'X' and labels 'Y' and store the trained classifier\n",
    "Print the type of the estimator\n",
    "Calculate the confusion matrix 'cm' between labels 'Y' and predicted labels 'y_pred'\n",
    "Print the confusion matrix\n",
    "Print the total sum of the confusion matrix\n",
    "Print the accuracy of the predictions, calculated as the trace of the confusion matrix divided by the total sum of the confusion matrix\n",
    "Print the Matthews correlation coefficient between labels 'Y' and predicted labels 'y_pred'\n",
    "Print the mean and standard deviation of the cross-validation scores, multiplied by 100 to get percentages\n",
    "Open the file \"../../Results/discriminatorResults.txt\" in append mode as 'f'\n",
    "Write the true positives, false positives, false negatives, and true negatives from the confusion matrix to the file\n",
    "Close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(0,10):\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=32, batch_size=200, verbose=2)\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "y_pred = cross_val_predict(estimator, X, dummy_y, cv=kfold)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "\n",
    "trained_classifier = estimator.fit(X, Y)\n",
    "print(type(estimator))\n",
    "\n",
    "cm = confusion_matrix(Y, y_pred)\n",
    "print(cm)\n",
    "print(\"total: \" + str(cm.sum()))\n",
    "print(\"accuracy: \" + str(np.trace(cm) / cm.sum()))\n",
    "print(\"Matthews correlation coefficient: \" + str(matthews_corrcoef(Y, y_pred)))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "f = open(\"../../Results/discriminatorResults.txt\", \"a+\")\n",
    "f.write(\"TP: %d, FP: %d, FN: %d, TN: %d\\n\" % (cm[0][0], cm[0][1], cm[1][0], cm[1][1]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start an infinite loop:\n",
    "    Generate random integers 'gen_hidden1', 'gen_hidden2', and 'gen_hidden3' between 1 and 101\n",
    "    Initialize a counter 'i' to 0\n",
    "    Start a loop that runs 100 times:\n",
    "        Define a unique result filename 'result_filename' using 'gen_hidden1', 'gen_hidden2', 'gen_hidden3', and 'i'\n",
    "        Train the GAN with 'gen_hidden1', 'gen_hidden2', and 'gen_hidden3'\n",
    "        Load the generated attacks 'results' from a file\n",
    "        Predict the labels 'y_pred' for the generated attacks using the estimator\n",
    "        Print 'y_pred'\n",
    "        Create labels 'portsweep_labels' for the generated attacks\n",
    "        Convert the predicted labels 'y_pred' to string labels 'predicted_as_label'\n",
    "        Get the unique labels 'unique_labels' in 'predicted_as_label'\n",
    "        For each label in 'unique_labels':\n",
    "            Print the label and its count in 'predicted_as_label'\n",
    "        Print a newline\n",
    "        Calculate the confusion matrix 'cm' between 'portsweep_labels' and 'y_pred'\n",
    "        Calculate the accuracy as the trace of 'cm' divided by the sum of 'cm'\n",
    "        Print 'cm', the total sum of 'cm', and the accuracy\n",
    "        If the accuracy is greater than 0.5:\n",
    "            Open a file in append mode and write the accuracy, 'gen_hidden1', 'gen_hidden2', 'gen_hidden3', 'i', and 'result_filename' to the file\n",
    "            Close the file\n",
    "            Define a new result filename 'result_filename' with a directory prefix\n",
    "            Open 'result_filename' in write mode and close it\n",
    "            Save 'results' to 'result_filename'\n",
    "        Increment 'i' by 1\n",
    "End infinite loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(1):\n",
    "    # generate random numbers for the hidden layer sizes of our generator\n",
    "    gen_hidden1 =  np.random.randint(1, 101)\n",
    "    gen_hidden2 =  np.random.randint(1, 101)\n",
    "    gen_hidden3 =  np.random.randint(1, 101)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    \n",
    "    # train 5 times on each setup, in case we get unlucky initalization on an otherwise good setup\n",
    "    while i < 100:\n",
    "        # create a unique filename in case we want to store the results (good accuracy)\n",
    "        result_filename = \"../../Results/GANresultsportsweep%.0f%.0f%.0fiter%.0ftry2.txt\" % (gen_hidden1, gen_hidden2, gen_hidden3, i)\n",
    "\n",
    "        trainGAN(gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "        \n",
    "        # load generate attacks from file\n",
    "        results = np.loadtxt(\"../../Results/GANresultsportsweep.txt\")\n",
    "\n",
    "        # predict attack lables (as encoded integers)\n",
    "        y_pred = estimator.predict(results)\n",
    "        print(y_pred)\n",
    "\n",
    "        # create appropriate labels for our generated portsweep attacks\n",
    "        portsweep_labels = np.full((len(results),), portsweep_index[0])\n",
    "\n",
    "        # convert integer labels back to string, get all unique strings and their count\n",
    "        predicted_as_label = attack_labels[y_pred]\n",
    "        unique_labels = np.unique(predicted_as_label)\n",
    "\n",
    "        for label in unique_labels:\n",
    "            print(\"Attack type: %s     number predicted:  %.0f\" % (label, len(np.where(predicted_as_label == label)[0])))\n",
    "    \n",
    "        print()\n",
    "        # create a confusion matrix of the results\n",
    "        cm = confusion_matrix(portsweep_labels, y_pred)\n",
    "        \n",
    "        accuracy = np.trace(cm) / cm.sum()\n",
    "        print(cm)\n",
    "        print(\"total: \" + str(cm.sum()))\n",
    "        print(\"accuracy: \" + str(accuracy))\n",
    "        \n",
    "        if accuracy > .50:\n",
    "            f = open(\"../../Results/GeneratorHypersAbove50percentAccuracyportsweep.txt\", \"a\")\n",
    "            f.write(\"\"\"\n",
    "            \n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "Accuracy: %.3f\n",
    "Generator hidden layer 1 size: %.0f\n",
    "Generator hidden layer 2 size: %.0f\n",
    "Generator hidden layer 3 size: %.0f\n",
    "Iteration %.0f\n",
    "Result file name: %s\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\" % (accuracy, gen_hidden1, gen_hidden2, gen_hidden3, i, result_filename))\n",
    "            f.close()\n",
    "            result_filename = \"../../Results/\" + result_filename\n",
    "            \n",
    "            f = open(result_filename, \"w\")\n",
    "            f.close()\n",
    "            np.savetxt(result_filename, results, fmt=\"%.0f\")\n",
    "        \n",
    "        i = i + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
